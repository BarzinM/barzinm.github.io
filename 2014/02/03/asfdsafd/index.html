<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<<<<<<< HEAD
<head>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Asfdsafd</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Portfolio of Barzin Moridian. Areas of interest: Robotics, Mechatronics, Controls, Machine Learning, and Image processing.">
    <link rel="canonical" href="http://barzinm.com/2014/02/03/asfdsafd/">
     <link rel="icon" href="favicon.png" type="image/gif"> 

    <!-- Custom CSS & Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link rel="stylesheet" href="http://barzinm.com/style.css">

    <!-- Custom Fonts -->
    <link rel="stylesheet" href="http://barzinm.com/css/font-awesome/css/font-awesome.min.css">
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>


</head>

<body id="page">
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-shrink" style="position:inherit">
    <div class="container">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand page-scroll" href="#page-top" ng-hide="">Barzin M</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li class="hidden">
                    <a href="#page-top"></a>
                </li>
                <li>
                    <a class="page-scroll" href="http://barzinm.com">Home</a>
                </li>
                <li>
                    <a class="page-scroll" href="#portfolio">Portfolio</a>
                </li>
                <li>
                    <a class="page-scroll" href="#contact">Contact</a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container-fluid -->
</nav>
<!-- Header --> 
=======
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Asfdsafd</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Portfolio of Barzin Moridian. Areas of interest: Robotics, Mechatronics, Controls, Machine Learning, and Image processing.">
    <link rel="canonical" href="http://barzinm.com/2014/02/03/asfdsafd/">
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<link href='http://fonts.googleapis.com/css?family=Damion' rel='stylesheet' type='text/css'>
<body id="page-top" class="index">


    <!-- Custom CSS & Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link rel="stylesheet" href="http://barzinm.com/../../../../postStyle.css">

    <!-- Custom Fonts -->
    <link rel="stylesheet" href="http://barzinm.com/css/font-awesome/css/font-awesome.min.css">
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body id="page-top" style="max-width:900px;margin:auto;background-color:#272822;">
<div class="postnav">
<!-- Navigation -->
<nav class="navbar navbar-default navbar-shrink" style="position:inherit">
    <div class="container">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand page-scroll" href="http://barzinm.com#page-top" ng-hide="">Barzin M</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li class="hidden">
                    <a href="#page-top"></a>
                </li>
                <li>
                    <a class="page-scroll" href="http://barzinm.com">Home</a>
                </li>
                <li>
                    <a class="page-scroll" href="http://barzinm.com#posts">Posts</a>
                </li>
                <li>
                    <a class="page-scroll" href="http://barzinm.com#portfolio">Portfolio</a>
                </li>
                <li>
                    <a class="page-scroll" href="http://barzinm.com#contact">Contact</a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container-fluid -->
</nav>
<!-- Header --> 
</div>
<div class="postcontent">
>>>>>>> b567af9b7239615f93ddfd14db18fa9344888715
<p>http://www.pelagodesign.com/blog/2009/07/21/how-to-make-ubuntu-linux-run-faster-on-a-laptop/
http://ubuntuforums.org/showthread.php?t=1062261
http://askubuntu.com/questions/196603/how-to-remove-the-graphical-user-interface</p>

<h1 id="linear-regression-method-for-machine-learning">Linear Regression Method for Machine learning</h1>
<ul id="markdown-toc">
  <li><a href="#linear-regression-method-for-machine-learning">Linear Regression Method for Machine learning</a>    <ul>
      <li><a href="#introduction">Introduction</a>        <ul>
          <li><a href="#hypothesis">Hypothesis</a></li>
          <li><a href="#gradient-descent">Gradient descent</a></li>
          <li><a href="#normal-equations">Normal equations</a></li>
        </ul>
      </li>
      <li><a href="#implementation">Implementation</a>        <ul>
          <li><a href="#code">Code</a></li>
          <li><a href="#training-set">Training set</a></li>
          <li><a href="#results">Results</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>Linear regression method is used to predict a value based on the information from a data set. In this method a relationship between a dependent variable and one or more explanatory variables is derived. The explanatory variables are called features. Value of features in study can be independent from each other or be a combination of other features. For example, for predicting the power load on a power plant the following features can be considered:</p>

<ul>
  <li>Constant value of <script type="math/tex">X_0=1</script></li>
  <li>Time of the day</li>
  <li>Time of the year</li>
  <li>Town population</li>
  <li>e<sup>age of power infrastructure</sup></li>
  <li>Number of houses</li>
  <li><script type="math/tex"> \frac{Town Population}{Number Of Houses} </script></li>
  <li>â€¦</li>
</ul>

<h3 id="hypothesis">Hypothesis</h3>
<p>Hypothesis is a function that uses some values (<script type="math/tex">X_0, X_1, X_2, X_3, ...</script>) to output a value <script type="math/tex">y</script> as a prediction.
Each feature <script type="math/tex"> X </script> is multiplied to a parameter <script type="math/tex">\theta</script> to construct a hypothesis. The hypothesis (e.g. power load prediction) has the following linear form:
<script type="math/tex">h_\theta (x) = \theta^T X</script>
Using optimization methods the best <script type="math/tex">\theta</script> values are calculated in a way that generate minimum errors from training set.
Two main methods of calculating parameters <script type="math/tex">\theta</script> are:</p>

<h3 id="gradient-descent">Gradient descent</h3>

<p>In gradient descent method, a cost function is computed that is equal to the average of differences (error) square between predicted values by hypothesis and actual values in the training set. </p>

<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})^2</script>

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="n">J</span> <span class="p">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>	<span class="c">% Cost function calculation</span></code></pre></div>

<p>In which <script type="math/tex">J</script> is the cost value, <script type="math/tex">y</script> is vector of  dependent values from the training set, and <script type="math/tex">m</script> is number of training samples.
The gradient of this cost function shows the best direction for minimizing relative to each parameter.</p>

<script type="math/tex; mode=display">\theta_j := \theta_j-\frac{\alpha}{m}\sum_{i=1}^{m} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script>

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="n">sum</span> <span class="p">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="n">X</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">);</span>	<span class="c">% Should be the same size of theta</span>
<span class="n">theta</span> <span class="p">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">sum</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>	<span class="c">% Updated theta values</span></code></pre></div>

<p>In which <script type="math/tex">\alpha</script>(<code>alpha</code>) is the learning rate set by the user.
This method requires feature normalization to reduce the number of iterations needed. This is done by subtracting the mean value of each feature group in the training set and dividing by their respective standard deviation.</p>

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="n">mu</span> <span class="p">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>	<span class="c">% Average value</span>
<span class="n">sigma</span> <span class="p">=</span> <span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>	<span class="c">% Standard deviation</span>
<span class="n">X_norm</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">./</span> <span class="n">sigma</span><span class="p">;</span>	<span class="c">% Normalized values</span></code></pre></div>

<h3 id="normal-equations">Normal equations</h3>

<p>This method gives the exact solution in a single calculation for optimized parameters <script type="math/tex">\theta</script> and does not need any feature scaling (normalization). The downside of using this method is that in case of adding a training set to the existing one or even adding one more training data requires doing all the calculations from the beginning which can be process demanding.
Normal equation can be calculated as follow:</p>

<script type="math/tex; mode=display">\theta=(X^T X)^{-1} X^T y</script>

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="n">theta</span> <span class="p">=</span> <span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">\</span> <span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span><span class="p">);</span> <span class="c">% Optimal theta values</span></code></pre></div>

<h2 id="implementation">Implementation</h2>

<p>In this section, the linear regression method is applied to a data set with 47 training samples each with 2 features.</p>

<h3 id="code">Code</h3>

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="n">clear</span> <span class="p">;</span> <span class="n">close</span> <span class="n">all</span><span class="p">;</span> <span class="n">clc</span>

<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;Loading data ...\n&#39;</span><span class="p">);</span>

<span class="c">%% Load Data</span>
<span class="n">data</span> <span class="p">=</span> <span class="n">load</span><span class="p">(</span><span class="s">&#39;training_set.txt&#39;</span><span class="p">);</span>
<span class="n">X</span> <span class="p">=</span> <span class="n">data</span><span class="p">(:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">);</span>
<span class="n">y</span> <span class="p">=</span> <span class="n">data</span><span class="p">(:,</span> <span class="mi">3</span><span class="p">);</span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>

<span class="c">% Print out some data points</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;First 10 examples from the dataset: \n&#39;</span><span class="p">);</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39; x = [%.0f %.0f], y = %.0f \n&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="mi">10</span><span class="p">,:)</span> <span class="n">y</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="mi">10</span><span class="p">,:)]</span><span class="o">&#39;</span><span class="p">);</span>

<span class="c">% Scale features and normalization</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;Normalizing Features ...\n&#39;</span><span class="p">);</span>

<span class="p">[</span><span class="n">X</span> <span class="n">mu</span> <span class="n">sigma</span><span class="p">]</span> <span class="p">=</span> <span class="n">featureNormalize</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>

<span class="c">% Add intercept term to X</span>
<span class="n">X</span> <span class="p">=</span> <span class="p">[</span><span class="nb">ones</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="n">X</span><span class="p">];</span>

<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;Running gradient descent ...\n&#39;</span><span class="p">);</span>

<span class="c">% Choose some alpha value</span>
<span class="n">alpha</span> <span class="p">=</span> <span class="mf">0.01</span><span class="p">;</span>
<span class="n">num_iters</span> <span class="p">=</span> <span class="mi">400</span><span class="p">;</span>

<span class="c">% Init Theta and Run Gradient Descent </span>
<span class="n">theta</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">J_history</span><span class="p">]</span> <span class="p">=</span> <span class="n">gradientDescentMulti</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">);</span>
<span class="c">% Plot the convergence graph</span>
<span class="n">figure</span><span class="p">;</span>
<span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="nb">numel</span><span class="p">(</span><span class="n">J_history</span><span class="p">),</span> <span class="n">J_history</span><span class="p">,</span> <span class="s">&#39;-b&#39;</span><span class="p">,</span> <span class="s">&#39;LineWidth&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;Number of iterations&#39;</span><span class="p">);</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Cost J&#39;</span><span class="p">);</span>

<span class="c">% Display gradient descent&#39;s result</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;Theta computed from gradient descent: \n&#39;</span><span class="p">);</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39; %f \n&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="p">);</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;\n&#39;</span><span class="p">);</span>

<span class="n">mu</span> <span class="p">=</span> <span class="p">[</span><span class="mi">0</span> <span class="n">mu</span><span class="p">];</span>
<span class="n">sigma</span> <span class="p">=</span> <span class="p">[</span><span class="mi">1</span> <span class="n">sigma</span><span class="p">];</span>
<span class="n">x_sample</span> <span class="p">=</span> <span class="p">([</span><span class="mi">1</span> <span class="mi">1650</span> <span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">./</span> <span class="n">sigma</span><span class="p">;</span>

<span class="n">y</span> <span class="p">=</span> <span class="n">x_sample</span> <span class="o">*</span> <span class="n">theta</span><span class="p">;</span> <span class="c">% You should change this</span>

<span class="n">fprintf</span><span class="p">([</span><span class="s">&#39;Predicted y (using gradient descent):\n $%f\n&#39;</span><span class="p">],</span> <span class="n">y</span><span class="p">);</span>

<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;Solving with normal equations...\n&#39;</span><span class="p">);</span>

<span class="n">data</span> <span class="p">=</span> <span class="n">csvread</span><span class="p">(</span><span class="s">&#39;ex1data2.txt&#39;</span><span class="p">);</span>
<span class="n">X</span> <span class="p">=</span> <span class="n">data</span><span class="p">(:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">);</span>
<span class="n">y</span> <span class="p">=</span> <span class="n">data</span><span class="p">(:,</span> <span class="mi">3</span><span class="p">);</span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>

<span class="c">% Add intercept term to X</span>
<span class="n">X</span> <span class="p">=</span> <span class="p">[</span><span class="nb">ones</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="n">X</span><span class="p">];</span>

<span class="c">% Calculate the parameters from the normal equation</span>
<span class="n">theta</span> <span class="p">=</span> <span class="n">normalEqn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>

<span class="c">% Display normal equation&#39;s result</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;Theta computed from the normal equations: \n&#39;</span><span class="p">);</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39; %f \n&#39;</span><span class="p">,</span> <span class="n">theta</span><span class="p">);</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;\n&#39;</span><span class="p">);</span>

<span class="n">x_sample</span> <span class="p">=</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">1650</span> <span class="mi">3</span><span class="p">];</span>
<span class="n">y</span> <span class="p">=</span> <span class="n">x_sample</span> <span class="o">*</span> <span class="n">theta</span><span class="p">;</span> 

<span class="n">fprintf</span><span class="p">([</span><span class="s">&#39;Predicted y (using normal equations):\n $%f\n&#39;</span><span class="p">],</span> <span class="n">y</span><span class="p">);</span></code></pre></div>

<hr />

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="k">function</span><span class="w"> </span>[X_norm, mu, sigma] <span class="p">=</span><span class="w"> </span><span class="nf">featureNormalize</span><span class="p">(</span>X<span class="p">)</span><span class="w"></span>
<span class="c">%FEATURENORMALIZE Normalizes the features in X </span>
<span class="c">%   FEATURENORMALIZE(X) returns a normalized version of X where</span>
<span class="c">%   the mean value of each feature is 0 and the standard deviation</span>
<span class="c">%   is 1. This is often a good preprocessing step to do when</span>
<span class="c">%   working with learning algorithms.</span>

<span class="n">X_norm</span> <span class="p">=</span> <span class="n">X</span><span class="p">;</span>
<span class="n">mu</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>
<span class="n">sigma</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">size</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>     

<span class="n">mu</span> <span class="p">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="n">sigma</span> <span class="p">=</span> <span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="n">X_norm</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">./</span> <span class="n">sigma</span><span class="p">;</span>

<span class="k">end</span></code></pre></div>

<hr />

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="k">function</span><span class="w"> </span>[theta, J_history] <span class="p">=</span><span class="w"> </span><span class="nf">gradientDescentMulti</span><span class="p">(</span>X, y, theta, alpha, num_iters<span class="p">)</span><span class="w"></span>
<span class="c">%GRADIENTDESCENTMULTI Performs gradient descent to learn theta</span>
<span class="c">%   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by</span>
<span class="c">%   taking num_iters gradient steps with learning rate alpha</span>

<span class="c">% Initialize values</span>
<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>
<span class="n">J_history</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">num_iters</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

<span class="k">for</span> <span class="n">iter</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">num_iters</span>

    <span class="c">% Save the cost J in every iteration    </span>
    <span class="n">J_history</span><span class="p">(</span><span class="n">iter</span><span class="p">)</span> <span class="p">=</span> <span class="n">computeCostMulti</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">);</span>

<span class="k">end</span>

<span class="k">end</span></code></pre></div>

<hr />

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="k">function</span><span class="w"> </span>J <span class="p">=</span><span class="w"> </span><span class="nf">computeCostMulti</span><span class="p">(</span>X, y, theta<span class="p">)</span><span class="w"></span>
<span class="c">%COMPUTECOSTMULTI Compute cost for linear regression with multiple variables</span>
<span class="c">%   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the</span>
<span class="c">%   parameter for linear regression to fit the data points in X and y</span>

<span class="n">m</span> <span class="p">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c">% number of training examples</span>

<span class="n">J</span> <span class="p">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">m</span><span class="p">;</span>

<span class="k">end</span></code></pre></div>

<hr />

<div class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="k">function</span><span class="w"> </span>[theta] <span class="p">=</span><span class="w"> </span><span class="nf">normalEqn</span><span class="p">(</span>X, y<span class="p">)</span><span class="w"></span>
<span class="c">%NORMALEQN Computes the closed-form solution to linear regression </span>
<span class="c">%   NORMALEQN(X,y) computes the closed-form solution to linear </span>
<span class="c">%   regression using the normal equations.</span>

<span class="n">theta</span> <span class="p">=</span> <span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">\</span> <span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span><span class="p">);</span>

<span class="k">end</span></code></pre></div>

<hr />

<h3 id="training-set">Training set</h3>

<p>Visualization of data:
<img src="https://dl.dropboxusercontent.com/u/9059775/blog/Machine_Learning/Linear_regression_data.jpg" alt="Data Visualization" /></p>

<p>Few numerical samples of data:</p>

<p>x = [2104 3], y = 399900 </p>

<p>x = [1600 3], y = 329900 </p>

<p>x = [2400 3], y = 369000 </p>

<p>x = [1416 2], y = 232000 </p>

<p>x = [3000 4], y = 539900 </p>

<p>x = [1985 4], y = 299900 </p>

<p>x = [1534 3], y = 314900 </p>

<p>x = [1427 3], y = 198999 </p>

<p>x = [1380 3], y = 212000 </p>

<p>x = [1494 3], y = 242500 </p>

<h3 id="results">Results</h3>

<p>Decreasing the cost function via gradient descent can be seen in the figure below:</p>

<p><img src="https://dl.dropboxusercontent.com/u/9059775/blog/Machine_Learning/LinearRegressionCost.jpg" alt="Cost function" /></p>

<p>And the final results are:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">Â </th>
      <th>Gradient descent</th>
      <th>Normal equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right"><script type="math/tex">\theta_1</script></td>
      <td>334302.063993</td>
      <td>89597.909543</td>
    </tr>
    <tr>
      <td style="text-align: right"><script type="math/tex">\theta_2</script></td>
      <td>100087.116006</td>
      <td>139.210674</td>
    </tr>
    <tr>
      <td style="text-align: right"><script type="math/tex">\theta_3</script></td>
      <td>3673.548451</td>
      <td>-8738.019112</td>
    </tr>
    <tr>
      <td style="text-align: right">Predicted <script type="math/tex">y</script> for <script type="math/tex">x = [1650, 3]</script></td>
      <td>289314.620338</td>
      <td>293081.464335</td>
    </tr>
  </tbody>
</table>

<p><strong>DO NOT FORGET</strong> that for calculating <script type="math/tex">y</script> using gradient descent method, <script type="math/tex">x=[1650, 3]</script> should go through the same normalization process as rest of the data. This is the main reason for the big difference between <script type="math/tex">\theta</script> values in two methods.</p>
        
</div>
  <footer>
        <div >
            <div class="row">
                <div class="col-md-4">
                    <span class="copyright">Copyright &copy; Barzin M 2014</span>
                </div>
                <div class="col-md-4">
                </div>
                <div class="col-md-4">
                    <ul class="list-inline quicklinks">
                        <li><a href="#">Privacy Policy</a>
                        </li>
                        <li><a href="#">Terms of Use</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </footer>
</body>
</html>

